# -*- coding: utf-8 -*-
"""N_Q_Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rs4IdbscoNYZAmr6FTLp-KJMtbs2wPSf

# This script should be run in Google colab: https://colab.research.google.com/
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
#
from sklearn.model_selection import train_test_split
from sklearn import preprocessing                    
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.activations import relu
from sklearn.metrics import mean_squared_error
from keras import backend as K
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import StratifiedKFold

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# read the data and print the first five rows
data = pd.read_excel('Database_20220111.xlsx',skiprows= 1, usecols = 'B:N', sheet_name= 0)
data.head()

# X - features
X = data.values[:,:-5]
# Y - target values
Y = data.values[:,-5:]
print(X.shape,Y.shape)

# normalize X & Y data via MinMaxScaler()
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
sc_X = MinMaxScaler()
sc_Y = MinMaxScaler()
x_norm = sc_X.fit_transform(X)
#y_norm0 = sc_Y.fit_transform(Y[:,0].reshape(-1,1))
y_norm1 = sc_Y.fit_transform(Y[:,1].reshape(-1,1))
y_norm2 = sc_Y.fit_transform(Y[:,2].reshape(-1,1))
y_norm3 = sc_Y.fit_transform(Y[:,3].reshape(-1,1))
y_norm4 = sc_Y.fit_transform(Y[:,4].reshape(-1,1))
y_norm = np.concatenate((y_norm1,y_norm2,y_norm3,y_norm4),axis=1)
#y_norm = np.concatenate((y_norm0,y_norm1,y_norm2,y_norm3,y_norm4),axis=1)
# y_norm = sc_Y.fit_transform(Y)
X_train, X_test, y_train, y_test = train_test_split(x_norm,y_norm,test_size=0.30)

"""# Sweep Layer, Neuron"""

# This returns a multi-layer-perceptron model in Keras.
from sklearn.model_selection import KFold

def keras_model(num_hidden_layers, 
                    num_neurons_per_layer, 
                    dropout_rate, 
                    activation):
    # create the MLP model.
    
    # define the layers.
    inputs = keras.Input(shape=(X_train.shape[1],))  # input layer.
    x = layers.Dropout(dropout_rate)(inputs) # dropout on the weights.
    
    # Add the hidden layers.
    for i in range(num_hidden_layers):
        x = layers.Dense(num_neurons_per_layer, 
                         activation=activation)(x)
        x = layers.Dropout(dropout_rate)(x)
    
    # output layer.
    outputs = layers.Dense(4, activation='linear')(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    optimizer = 'adam'
    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.MeanSquaredError(),
                  metrics=['mse'])
    return model

# cross validation
data1 = X_train; labels1 = y_train
folds = 5
layer_loop = 5
neuron_loop = 15
kfold = KFold(n_splits=folds, shuffle=True, random_state=101)
loss_mtx = [[] for _ in range(folds)]

for layer in range(1,1+layer_loop):
  for neuron in range(2,2*neuron_loop+2,2):
    print("numLayer=",layer," | numNeuron=",neuron," is running ...") 
    fold_k = 0
    for train,test in kfold.split(data1, labels1):
      trainX, testX = data1[train], data1[test]
      trainY, testY = labels1[train], labels1[test]
      model = keras_model(layer,neuron,0.0,'tanh')
      res = model.fit(trainX, trainY,
                    validation_data=(testX,testY),
                    epochs=21,
                    verbose=0)
      min_score = np.array(res.history['val_loss']).min()
      loss_mtx[fold_k].append(min_score)
      fold_k += 1
# save as csv file
# losses = np.array(loss_mtx).reshape(5,15)
# np.savetxt('losses.csv',losses,delimiter=",")
for i in range(folds):
  losses = np.array(loss_mtx[i]).reshape(layer_loop,neuron_loop)
  np.savetxt('losses_{}.csv'.format(i),losses,delimiter=",")
  print("Fold",i," is recorded ^_^") 

"""# Re-run NN model using identified optimal N, Q"""

# This returns a multi-layer-perceptron model in Keras.
from sklearn.model_selection import KFold

def keras_model(num_hidden_layers, 
                    num_neurons_per_layer, 
                    dropout_rate, 
                    activation):
    # create the MLP model.
    
    # define the layers.
    inputs = keras.Input(shape=(X_train.shape[1],))  # input layer.
    x = layers.Dropout(dropout_rate)(inputs) # dropout on the weights.
    
    # Add the hidden layers.
    for i in range(num_hidden_layers):
        x = layers.Dense(num_neurons_per_layer, 
                         activation=activation)(x)
        x = layers.Dropout(dropout_rate)(x)
    
    # output layer.
    outputs = layers.Dense(4, activation='linear')(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    optimizer = 'adam'
    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.MeanSquaredError(),
                  metrics=['mse'])
    return model

layer = 2
neuron = 12
model = keras_model(layer,neuron,0.0,'tanh')
print("All Done!")